# =============================================================================
# POLARIS World Model Configuration: Gemini Implementation
# =============================================================================
# This file defines the configuration for the 'gemini' World Model.
# It adheres to the structure expected by the ConfigurationValidator.

# [REQUIRED] Specifies the active World Model implementation.
# Must be a registered type in the World Model factory.
implementation: "gemini"

# [OPTIONAL] A boolean to control behavior on failure.
# If true, the system will attempt to reload the model if it enters a failed state.
reload_on_failure: true

# [OPTIONAL] A number specifying the health check interval in seconds.
# Defines how often the model's health and connectivity are checked.
health_check_interval_sec: 30

# [RECOMMENDED] Implementation-specific settings for the 'gemini' model.
# This entire block is passed to the Gemini World Model for its internal setup.
config:
  # -------------------------------------------------------------------------
  # Gemini LLM World Model - AI-Powered Reasoning
  # -------------------------------------------------------------------------
  # Google Gemini Large Language Model integration for advanced reasoning.
  # Provides natural language understanding, complex analysis, and contextual decision making.
  #
  # Use Cases: Complex analysis, natural language queries, root cause diagnosis.
  # Dependencies: Internet access, Google AI API, API key.
  # Cost: API calls are metered and billed by Google based on token usage.
  # -------------------------------------------------------------------------
  enabled: true
  description: "Google Gemini LLM-based World Model with LangChain integration"

  # --- API Configuration and Authentication ---
  # SECURITY: Never store API keys in configuration files. Use environment variables.
  api_key_env: "GEMINI_API_KEY"  # Environment variable containing the API key.

  # Model selection affects capabilities and cost.
  # "gemini-2.5-flash": Fast, cost-effective, good for most use cases.
  # "gemini-2.5-pro": More capable, higher cost, better for complex reasoning.
  model: "gemini-2.5-flash"

  # --- Generation Parameters - Control AI Behavior ---
  # Temperature controls randomness in responses (0.0 to 1.0).
  # 0.0=deterministic, 0.7=balanced, 1.0=creative.
  temperature: 0.7

  # Maximum tokens in the response, affecting cost and length.
  # 2048 is recommended for detailed responses.
  max_tokens: 2048

  # Top-p (nucleus sampling) controls response diversity (0.0 to 1.0).
  # 0.9 is a good balance.
  top_p: 0.9

  # Top-k limits the vocabulary for each token.
  # 40 provides a good balance between coherence and variety.
  top_k: 40

  # --- Performance and Reliability Settings ---
  # Maximum concurrent API requests to Google. Higher values may hit rate limits.
  concurrent_requests: 5

  # Timeout for individual API requests in seconds.
  request_timeout_sec: 30

  # Retry configuration for handling transient API or network failures.
  retry_attempts: 3
  retry_delay_sec: 1 # Initial delay, uses exponential backoff.

  # --- Memory and Learning Configuration ---
  # Configuration for conversation memory and meta-learning
  max_conversation_memory: 50
  max_history_events: 1000
  
  # Meta-learning parameters for continuous improvement
  learning_rate: 0.1  # Alpha for exponential moving average in calibration
  confidence_threshold: 0.6  # Minimum confidence for reliable predictions

  # --- Prompt Templates - Control AI Reasoning Behavior ---
  prompts:
    # System-level prompt that establishes the AI's persona and capabilities.
    system_prompt: |
      You are an expert system analyst for the POLARIS adaptive system framework.
      You have deep knowledge of system monitoring, performance analysis, and root cause diagnosis.
      You understand distributed systems, performance metrics, and adaptation strategies.
      
      Your responses should be:
      - Accurate and based on provided data
      - Actionable with specific recommendations
      - Clear and concise for technical audiences
      - Include confidence levels (0.0-1.0) for your assessments
      
      You learn from calibration feedback to improve your accuracy over time.
      Always consider system safety and stability in your recommendations.
      Provide confidence scores for all your assessments and explain your reasoning.

# Environment variable overrides for different deployment environments
# Values here will override the configuration above when the corresponding
# environment variable is set to 'true' (case-insensitive).
env_overrides:
  # Development environment overrides (LOCAL_DEV=true)
  LOCAL_DEV:
    config:
      model: "gemini-2.5-flash"  # Use faster, cheaper model in dev
      temperature: 0.3  # More deterministic responses
      max_tokens: 1024  # Shorter responses
      concurrent_requests: 2  # Lower concurrency
      request_timeout_sec: 10  # Shorter timeouts
      retry_attempts: 2  # Fewer retries
      langchain:
        verbose: true  # Enable verbose logging in dev

  # Test environment overrides (TEST_ENV=true)
  TEST_ENV:
    config:
      temperature: 0.1  # Very deterministic for consistent tests
      max_tokens: 512  # Shorter responses for testing
      concurrent_requests: 1  # Sequential processing
      retry_attempts: 1  # Fail fast in tests
      prompts:
        system_prompt: |
          [TEST MODE] You are in a test environment. Provide concise, deterministic responses.
          Focus on validating system behavior rather than comprehensive analysis.

  # Staging environment overrides (STAGING=true)
  STAGING:
    config:
      temperature: 0.5  # Balanced creativity for staging
      max_tokens: 2048  # Full response length
      concurrent_requests: 3  # Moderate concurrency
      request_timeout_sec: 45  # More generous timeout
      retry_attempts: 3  # Standard retries
      prompts:
        system_prompt: |
          [STAGING] You are in a staging environment. Provide detailed analysis
          but note this is not production. Be cautious with recommendations.

  # Production environment overrides (PRODUCTION=true)
  PRODUCTION:
    config:
      temperature: 0.7  # Default creative setting
      max_tokens: 2048  # Full response length
      concurrent_requests: 5  # Higher concurrency
      request_timeout_sec: 30  # Balanced timeout
      retry_attempts: 3  # Standard retries
      prompts:
        system_prompt: |
          [PRODUCTION] You are in a production environment. Provide accurate,
          reliable analysis with safety as the top priority. Be explicit about
          confidence levels and potential risks in your recommendations.

